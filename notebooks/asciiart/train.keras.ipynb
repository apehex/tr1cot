{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xXM7DoPpds1"
      },
      "source": [
        "## Import deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W24EKFXaO5yC"
      },
      "outputs": [],
      "source": [
        "!pip install -qq tensorflow==2.18.0\n",
        "!pip install -qq tensorflow-tpu==2.18.0 --find-links=https://storage.googleapis.com/libtpu-tf-releases/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkXY6AeYod0Y"
      },
      "outputs": [],
      "source": [
        "!pip install -qq -U datasets mlable tokun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKU9BSWyhcX0"
      },
      "outputs": [],
      "source": [
        "!pip install -qq --no-index -f '/content/libs/' tr1cot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXU-Ebl2pddk"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import functools\n",
        "import itertools\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import urllib.request\n",
        "\n",
        "import datasets as hd\n",
        "import tensorflow as tf\n",
        "\n",
        "import mlable.data\n",
        "import mlable.metrics\n",
        "import mlable.sampling\n",
        "import mlable.shapes\n",
        "import mlable.shaping.axes\n",
        "import mlable.shaping.hilbert\n",
        "import mlable.text\n",
        "\n",
        "import tokun.data\n",
        "import tokun.eval\n",
        "import tokun.models.klvae\n",
        "import tokun.models.vqvae\n",
        "import tokun.pipeline.flat.preprocess\n",
        "import tokun.pipeline.flat.postprocess\n",
        "import tokun.pipeline.hilbert.preprocess\n",
        "import tokun.pipeline.hilbert.postprocess\n",
        "import tokun.pipeline.square.preprocess\n",
        "import tokun.pipeline.square.postprocess\n",
        "\n",
        "import tr1cot.models.cnn\n",
        "import tr1cot.models.vit\n",
        "import tr1cot.models.unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pn1ywhSrpin9"
      },
      "outputs": [],
      "source": [
        "print(\"Tensorflow version \" + tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Setup the GPU / TPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CR5vpwQGCPl8"
      },
      "outputs": [],
      "source": [
        "# DEBUGGING ####################################################################\n",
        "\n",
        "tf.keras.config.disable_traceback_filtering()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWMvVNSAJK_n"
      },
      "outputs": [],
      "source": [
        "# MIXED PRECISION ##############################################################\n",
        "\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_bfloat16')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFIMfPmgQa0h"
      },
      "outputs": [],
      "source": [
        "# DEVICES ######################################################################\n",
        "\n",
        "tf.debugging.set_log_device_placement(False)\n",
        "\n",
        "CPU = tf.config.list_logical_devices('CPU')\n",
        "GPU = tf.config.list_logical_devices('GPU')\n",
        "TPU = tf.config.list_logical_devices('TPU')\n",
        "\n",
        "if TPU:\n",
        "    RESOLVER = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')\n",
        "    tf.config.experimental_connect_to_cluster(RESOLVER)\n",
        "    tf.tpu.experimental.initialize_tpu_system(RESOLVER)\n",
        "    DISTRIBUTION_STRATEGY = tf.distribute.TPUStrategy(RESOLVER)\n",
        "elif GPU:\n",
        "    DISTRIBUTION_STRATEGY = tf.distribute.MirroredStrategy(GPU)\n",
        "else:\n",
        "    DISTRIBUTION_STRATEGY = tf.distribute.MirroredStrategy(CPU)\n",
        "\n",
        "print('CPU: ', CPU)\n",
        "print('GPU: ', GPU)\n",
        "print('TPU: ', TPU)\n",
        "print('DS: ', DISTRIBUTION_STRATEGY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9066X5EOyAX"
      },
      "source": [
        "## Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFSPMtQaO1fu"
      },
      "outputs": [],
      "source": [
        "# TOGGLE #######################################################################\n",
        "\n",
        "IMPORT = False\n",
        "DOWNLOAD = False\n",
        "TRAINING = True\n",
        "RANDOM = True\n",
        "\n",
        "DATA = 'square' # 'flat' / 'hilbert' / 'square'\n",
        "ARCH0 = 'vqvae' # 'klvae' / 'vqvae'\n",
        "ARCH1 = 'unet' # 'vit' / 'cnn' / 'unet'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t1jfsJlM3SX"
      },
      "source": [
        "## Defining The Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfHiXTM8WGE6"
      },
      "outputs": [],
      "source": [
        "# COMMON PARAMETERS ############################################################\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    'batch_dim': 32, # B\n",
        "    'token_dim': 3, # T\n",
        "    'drop_dim': 1, # D, number of bytes dropped from the encoding\n",
        "    'input_dim': 256, # U_i (bytes)\n",
        "    'height_dim': 64, # H\n",
        "    'width_dim': 64 * 4, # W * (T + D)\n",
        "    'sample_dim': 1024, # S = L * (T + D)\n",
        "    'order_num': 5, # O => H = W = 2 ** O\n",
        "    'rank_num': 2, # R\n",
        "    'start_rate': 0.98,\n",
        "    'end_rate': 0.02,\n",
        "    'epochs': 32,\n",
        "    'steps': 2 ** 7,\n",
        "    'epsilon': 1e-6,\n",
        "    'dropout': 0.01,\n",
        "    'trainable': False, # whether to freeze the weight of tokun\n",
        "    'bigendian': True,\n",
        "    'encoding': 'UTF-32-BE',}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHjbusZbOUSI"
      },
      "outputs": [],
      "source": [
        "# TOKUN PARAMETERS #############################################################\n",
        "\n",
        "TOKUN_FACTORY = {\n",
        "    'klvae': tokun.models.klvae.KlAutoEncoder,\n",
        "    'vqvae': tokun.models.vqvae.QuantizedAutoEncoder,}\n",
        "\n",
        "TOKUN_CONFIG = {\n",
        "    'vqvae': {\n",
        "        'token_dim': BASE_CONFIG['token_dim'],\n",
        "        'input_dim': BASE_CONFIG['input_dim'],\n",
        "        'embed_dim': 64,\n",
        "        'binary_dim': 8,},\n",
        "    'klvae': {\n",
        "        'block_cfg': [\n",
        "            {'layer_num': 2, 'channel_dim': 64, 'group_dim': 32, 'head_dim': 32, 'head_num': 8, 'add_sampling': False, 'add_attention': False,},\n",
        "            {'layer_num': 2, 'channel_dim': 128, 'group_dim': 32, 'head_dim': 32, 'head_num': 16, 'add_sampling': True, 'add_attention': False,},\n",
        "            {'layer_num': 4, 'channel_dim': 256, 'group_dim': 64, 'head_dim': 64, 'head_num': 16, 'add_sampling': True, 'add_attention': True,},],\n",
        "        'input_dim': BASE_CONFIG['input_dim'],\n",
        "        'embed_dim': 64,\n",
        "        'output_dim': 8 * BASE_CONFIG['token_dim'],\n",
        "        'step_min': 0,\n",
        "        'step_max':  BASE_CONFIG['steps'],\n",
        "        'beta_min': 0.0001,\n",
        "        'beta_max': 0.01,\n",
        "        'dropout_rate': BASE_CONFIG['dropout'],\n",
        "        'epsilon_rate': BASE_CONFIG['epsilon'],},}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z74MlibMWnu"
      },
      "outputs": [],
      "source": [
        "# MODEL PARAMETERS #############################################################\n",
        "\n",
        "MODEL_FACTORY = {\n",
        "    'cnn': tr1cot.models.cnn.CnnDiffusionModel,\n",
        "    'vit': tr1cot.models.vit.VitDiffusionModel,\n",
        "    'unet': tr1cot.models.unet.UnetDiffusionModel,}\n",
        "\n",
        "MODEL_CONFIG = {\n",
        "    'cnn': {\n",
        "        'block_num': 4,\n",
        "        'latent_dim': [64, 128, 256],\n",
        "        'start_rate': BASE_CONFIG['start_rate'],\n",
        "        'end_rate': BASE_CONFIG['end_rate'],},\n",
        "    'vit': {\n",
        "        'patch_dim': [1, 1, 2, 2, 1, 1],\n",
        "        'start_rate': BASE_CONFIG['start_rate'],\n",
        "        'end_rate': BASE_CONFIG['end_rate'],\n",
        "        'dropout_rate': 0.01,},\n",
        "    'unet': {\n",
        "        'channel_dim': [64, 128, 128, 128, 192, 192],\n",
        "        'group_dim': None,\n",
        "        'head_dim': None,\n",
        "        'head_num': None,\n",
        "        'layer_num': 2,\n",
        "        'add_attention': [0, 0, 1, 1, 0, 0],\n",
        "        'add_downsampling': [0, 1, 1, 0, 0, 0],\n",
        "        'add_upsampling': [0, 0, 0, 1, 1, 0],\n",
        "        'start_rate': BASE_CONFIG['start_rate'],\n",
        "        'end_rate': BASE_CONFIG['end_rate'],\n",
        "        'dropout_rate': BASE_CONFIG['dropout'],\n",
        "        'epsilon_rate': BASE_CONFIG['epsilon'],},}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMA0OZAGdq-g"
      },
      "outputs": [],
      "source": [
        "# DERIVED MODEL PARAMETERS #####################################################\n",
        "\n",
        "META_CONFIG = {\n",
        "    'tokun': '{}.{}x{}'.format(ARCH0, BASE_CONFIG['token_dim'], TOKUN_CONFIG[ARCH0]['embed_dim']),\n",
        "    'tr1cot': '{}'.format(ARCH1),}\n",
        "\n",
        "IO_CONFIG = {\n",
        "    'tokun': {\n",
        "        'url': 'https://github.com/apehex/tokun/raw/main/models/{}.keras'.format(META_CONFIG['tokun']),\n",
        "        'path': 'tokun.keras',},\n",
        "    'tr1cot': {\n",
        "        'url': 'https://github.com/apehex/tr1cot/raw/main/models/{}.keras'.format(META_CONFIG['tr1cot']),\n",
        "        'path': 'tr1cot.keras',},}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4-naWbUdV4o"
      },
      "outputs": [],
      "source": [
        "# PREPROCESSING ################################################################\n",
        "\n",
        "ANSI_REGEX = r'\\x1b\\[[0-9;]*[mGKHF]'\n",
        "\n",
        "FILTER_CONFIG = {\n",
        "    'pattern': '.*',} # '.*[Cc]ats.*'\n",
        "\n",
        "BATCH_CONFIG = {\n",
        "    'batch_size': BASE_CONFIG['batch_dim'],\n",
        "    'drop_remainder': True,\n",
        "    'num_parallel_calls': tf.data.AUTOTUNE,}\n",
        "\n",
        "PIPELINE_FACTORY = {\n",
        "    'flat': tokun.pipeline.flat.preprocess.factory,\n",
        "    'hilbert': tokun.pipeline.hilbert.preprocess.factory,\n",
        "    'square': tokun.pipeline.square.preprocess.factory,}\n",
        "\n",
        "PIPELINE_CONFIG = {\n",
        "    'flat': {\n",
        "        'batch_dim': BATCH_CONFIG['batch_size'],\n",
        "        'token_dim': BASE_CONFIG['token_dim'],\n",
        "        'drop_dim': BASE_CONFIG['drop_dim'],\n",
        "        'sample_dim': (BASE_CONFIG['token_dim'] + BASE_CONFIG['drop_dim']) * BASE_CONFIG['sample_dim'],\n",
        "        'pattern': ANSI_REGEX,\n",
        "        'rewrite': '',\n",
        "        'separator': '\\u001d',\n",
        "        'encoding': BASE_CONFIG['encoding'],\n",
        "        'bigendian': BASE_CONFIG['bigendian'],\n",
        "        'targets': False,},\n",
        "    'hilbert': {\n",
        "        'batch_dim': BATCH_CONFIG['batch_size'],\n",
        "        'token_dim': BASE_CONFIG['token_dim'],\n",
        "        'order_num': BASE_CONFIG['order_num'],\n",
        "        'rank_num': BASE_CONFIG['rank_num'],\n",
        "        'pattern': ANSI_REGEX,\n",
        "        'rewrite': '',\n",
        "        'separator': '\\u001d',\n",
        "        'encoding': BASE_CONFIG['encoding'],\n",
        "        'bigendian': BASE_CONFIG['bigendian'],\n",
        "        'targets': False,},\n",
        "    'square': {\n",
        "        'batch_dim': BATCH_CONFIG['batch_size'],\n",
        "        'token_dim': BASE_CONFIG['token_dim'],\n",
        "        'drop_dim': BASE_CONFIG['drop_dim'],\n",
        "        'height_dim': BASE_CONFIG['height_dim'],\n",
        "        'width_dim': BASE_CONFIG['width_dim'],\n",
        "        'pattern': ANSI_REGEX,\n",
        "        'rewrite': '',\n",
        "        'separator': '\\u001d',\n",
        "        'encoding': BASE_CONFIG['encoding'],\n",
        "        'bigendian': BASE_CONFIG['bigendian'],\n",
        "        'targets': False,},}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0gg2JjoQYEN"
      },
      "outputs": [],
      "source": [
        "# POSTPROCESSING ###############################################################\n",
        "\n",
        "POSTPROCESSING_FACTORY = {\n",
        "    'flat': tokun.pipeline.flat.postprocess.factory,\n",
        "    'hilbert': tokun.pipeline.hilbert.postprocess.factory,\n",
        "    'square': tokun.pipeline.square.postprocess.factory,}\n",
        "\n",
        "POSTPROCESSING_CONFIG = {\n",
        "    'flat': {\n",
        "        'drop_dim': PIPELINE_CONFIG['flat']['drop_dim'],\n",
        "        'encoding': PIPELINE_CONFIG['flat']['encoding'],\n",
        "        'bigendian': PIPELINE_CONFIG['flat']['bigendian'],\n",
        "        'threshold': 0.0,\n",
        "        'errors': 'replace',},\n",
        "    'hilbert': {\n",
        "        'order_num': PIPELINE_CONFIG['hilbert']['order_num'],\n",
        "        'rank_num': PIPELINE_CONFIG['hilbert']['rank_num'],\n",
        "        'encoding': PIPELINE_CONFIG['hilbert']['encoding'],\n",
        "        'bigendian': PIPELINE_CONFIG['hilbert']['bigendian'],\n",
        "        'threshold': 0.0,\n",
        "        'errors': 'replace',},\n",
        "    'square': {\n",
        "        'drop_dim': PIPELINE_CONFIG['square']['drop_dim'],\n",
        "        'encoding': PIPELINE_CONFIG['square']['encoding'],\n",
        "        'bigendian': PIPELINE_CONFIG['square']['bigendian'],\n",
        "        'threshold': 0.0,\n",
        "        'errors': 'replace',},}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21J7WEkhSwph"
      },
      "outputs": [],
      "source": [
        "# TRAINING PARAMETERS ##########################################################\n",
        "\n",
        "TRAINING_CONFIG = {\n",
        "    'epochs': BASE_CONFIG['epochs'],\n",
        "    'batch_size': None,\n",
        "    'validation_split': None,\n",
        "    'validation_freq': list(range(1, 9)),\n",
        "    # 'class_weight': {__c: 1. if __c == 0 else 1. for __c in range(256)}, # there are 3 times more 0s than other bytes\n",
        "    'verbose': 1,}\n",
        "\n",
        "OPTIMIZER_CONFIG = {\n",
        "    'learning_rate': 0.0001 * (0.1 if IMPORT else 1.0),\n",
        "    'weight_decay': 0.000001,\n",
        "    'beta_1': 0.9,\n",
        "    'beta_2': 0.999,\n",
        "    'epsilon': 1e-7,\n",
        "    'clipnorm': 0.1,\n",
        "    'amsgrad': False,\n",
        "    'use_ema': False,\n",
        "    'ema_momentum': 0.99,\n",
        "    'ema_overwrite_frequency': BASE_CONFIG['steps'] // 8,}\n",
        "    # 'gradient_accumulation_steps': 2,}\n",
        "\n",
        "SCHEDULER_CONFIG = {\n",
        "    'initial_learning_rate': 0.0001 * OPTIMIZER_CONFIG['learning_rate'],\n",
        "    'decay_steps': TRAINING_CONFIG['epochs'] * BASE_CONFIG['steps'],\n",
        "    'alpha': 0.01,\n",
        "    'name': 'cosine_lr',\n",
        "    'warmup_target': OPTIMIZER_CONFIG['learning_rate'],\n",
        "    'warmup_steps': BASE_CONFIG['steps'] // 8,}\n",
        "\n",
        "LOSS_CONFIG = {\n",
        "    'from_logits': True,\n",
        "    'label_smoothing': 0.0,\n",
        "    'axis': -1,\n",
        "    'reduction': 'sum_over_batch_size',\n",
        "    'name': 'ce_loss',}\n",
        "\n",
        "METRICS_CONFIG = {\n",
        "    'depth': 8,\n",
        "    'from_logits': True,}\n",
        "\n",
        "CHECKPOINT_CONFIG = {\n",
        "    'filepath': IO_CONFIG['tr1cot']['path'],\n",
        "    'monitor': 'val_loss',\n",
        "    'mode': 'auto',\n",
        "    'save_freq': 'epoch',\n",
        "    'save_best_only': False,\n",
        "    'save_weights_only': False,\n",
        "    'verbose': 1,}\n",
        "\n",
        "TENSORBOARD_CONFIG = {\n",
        "    'log_dir': os.path.join('.logs/', META_CONFIG['tr1cot'], datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")),\n",
        "    'histogram_freq': 1,\n",
        "    'embeddings_freq': 0,\n",
        "    # 'profile_batch': (0, 4),\n",
        "    'write_graph': True,\n",
        "    'write_images': True,}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdzyDZfKnMw4"
      },
      "outputs": [],
      "source": [
        "# DATASETS #####################################################################\n",
        "\n",
        "DATASETS_CONFIG = {\n",
        "    # 'pt-fineweb-edu': {\n",
        "    #     'path': 'HuggingFaceFW/fineweb-edu',\n",
        "    #     'name': 'sample-10BT',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['text'],},\n",
        "    # 'pt-fineweb-kor': {\n",
        "    #     'path': 'HuggingFaceFW/fineweb-2',\n",
        "    #     'name': 'kor_Hang',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['text'],},\n",
        "    # 'pt-fineweb-fin': {\n",
        "    #     'path': 'HuggingFaceFW/fineweb-2',\n",
        "    #     'name': 'fin_Latn',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['text'],},\n",
        "    # 'pt-wikipedia': {\n",
        "    #     'path': 'wikimedia/wikipedia',\n",
        "    #     'name': '20231101.en',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['text'],},\n",
        "    # 'tp-wikipedia-1': {\n",
        "    #     'path': 'wikimedia/wikipedia',\n",
        "    #     'name': '20231101.en',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['text'],},\n",
        "    # 'tp-wikipedia-2': {\n",
        "    #     'path': 'wikimedia/wikipedia',\n",
        "    #     'name': '20231101.en',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['text'],},\n",
        "    # 'ft-retro-ascii-art': {\n",
        "    #     'path': 'jdpressman/retro-ascii-art-v1',\n",
        "    #     'name': None,\n",
        "    #     'train': 'train',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['prompt', 'art_aic'],},\n",
        "    # 'ft-stack-exchange': {\n",
        "    #     'path': 'Alignment-Lab-AI/Stack-Exchange-April',\n",
        "    #     'name': None,\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['question', 'answer'],},\n",
        "    # 'ft-math': {\n",
        "    #     'path': 'HuggingFaceTB/finemath',\n",
        "    #     'name': 'finemath-3plus',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['text'],},\n",
        "    # 'cot-text-dolphin': {\n",
        "    #     'path': 'cognitivecomputations/dolphin-r1',\n",
        "    #     'name': 'reasoning-deepseek',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['reasoning', 'answer'],},\n",
        "    # 'cot-text-openthoughts': {\n",
        "    #     'path': 'open-thoughts/OpenThoughts-114k',\n",
        "    #     'name': 'default',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['problem', 'solution'],},\n",
        "    # 'ft-asciiart-asciiart': {\n",
        "    #     'path': 'apehex/ascii-art',\n",
        "    #     'name': 'asciiart',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['content'],},\n",
        "    # 'ft-asciiart-copypasta': {\n",
        "    #     'path': 'apehex/ascii-art',\n",
        "    #     'name': 'copypasta',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['content'],},\n",
        "    # 'ft-asciiart-graffiti': {\n",
        "    #     'path': 'apehex/ascii-art',\n",
        "    #     'name': 'graffiti',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['content'],},\n",
        "    # 'ft-asciiart-images': {\n",
        "    #     'path': 'apehex/ascii-art',\n",
        "    #     'name': 'images',\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['content'],},\n",
        "    'ft-asciiart-datacompdr': {\n",
        "        'path': 'apehex/ascii-art-datacompdr-12m',\n",
        "        'name': 'default',\n",
        "        'split': 'fixed',\n",
        "        'features': ['content'],},\n",
        "    # 'cot-math-numi': {\n",
        "    #     'path': 'AI-MO/NuminaMath-CoT',\n",
        "    #     'name': None,\n",
        "    #     'split': 'train',\n",
        "    #     'features': ['problem', 'solution'],},\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--3gOXZgLtG0"
      },
      "outputs": [],
      "source": [
        "# PLOT #########################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNF00bM5xj9O"
      },
      "source": [
        "## Downloading The Model Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfIZb86Fg0dQ"
      },
      "outputs": [],
      "source": [
        "# IMPORT #######################################################################\n",
        "\n",
        "# tokun\n",
        "urllib.request.urlretrieve(IO_CONFIG['tokun']['url'], IO_CONFIG['tokun']['path'])\n",
        "\n",
        "# tr1cot\n",
        "if IMPORT and DOWNLOAD:\n",
        "    urllib.request.urlretrieve(IO_CONFIG['tr1cot']['url'], IO_CONFIG['tr1cot']['path'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEyFtkcFNGe4"
      },
      "source": [
        "## Downloading The Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTK1MPV8qek5"
      },
      "outputs": [],
      "source": [
        "# DOWNLOAD #####################################################################\n",
        "\n",
        "DATASETS = {\n",
        "    __name: hd.load_dataset(path=__args['path'], name=__args['name'], split=__args['split']).to_tf_dataset(shuffle=False, batch_size=None)\n",
        "    for __name, __args in DATASETS_CONFIG.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlo20AczsU1W"
      },
      "outputs": [],
      "source": [
        "# STATS ########################################################################\n",
        "\n",
        "STATS = {__n: mlable.data.stats(dataset=DATASETS[__n], features=DATASETS_CONFIG[__n]['features'], count=2048) for __n in DATASETS}\n",
        "\n",
        "print(STATS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_LXytNaVsnZ"
      },
      "outputs": [],
      "source": [
        "# VIZ ##########################################################################\n",
        "\n",
        "# __i = iter(DATASETS['ft-asciiart-datacompdr'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNENi7nQV4Th"
      },
      "outputs": [],
      "source": [
        "# __s = next(__i)\n",
        "# print(__s['caption'].numpy().decode('utf-8'), __s['labels'].numpy().decode('utf-8'), len(__s['content'].numpy().decode('utf-8')))\n",
        "# print(__s['content'].numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cheN52OEchs"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gp2WitYVhs8I"
      },
      "outputs": [],
      "source": [
        "# ITERATE ######################################################################\n",
        "\n",
        "# __filter = lambda __s: True\n",
        "# __filter = lambda __s: tf.strings.regex_full_match(__s['labels'], **FILTER_CONFIG)\n",
        "\n",
        "for __name in DATASETS:\n",
        "    # specialized preprocessing fn\n",
        "    __preprocess = PIPELINE_FACTORY[DATA](\n",
        "        features=DATASETS_CONFIG[__name]['features'],\n",
        "        **PIPELINE_CONFIG[DATA])\n",
        "    # apply\n",
        "    DATASETS[__name] = DATASETS[__name].batch(**BATCH_CONFIG).map(__preprocess, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pqt3YsjMQDy9"
      },
      "outputs": [],
      "source": [
        "# POSTPROCESS ##################################################################\n",
        "\n",
        "__postprocess_greedy = POSTPROCESSING_FACTORY[DATA](**POSTPROCESSING_CONFIG[DATA])\n",
        "__postprocess_sampler = POSTPROCESSING_FACTORY[DATA](temp=1.0, topp=0.9, topk=4, **POSTPROCESSING_CONFIG[DATA])\n",
        "__postprocess_probs = POSTPROCESSING_FACTORY[DATA](**{__k: (0.5 if __k == 'threshold' else __v) for __k, __v in POSTPROCESSING_CONFIG[DATA].items()})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnzCjLkrlI8d"
      },
      "outputs": [],
      "source": [
        "# CONCATENATE ##################################################################\n",
        "\n",
        "DATASET_KEYS = set(DATASETS.keys()) - {'random'}\n",
        "\n",
        "DATASET_ALL = functools.reduce(lambda __l, __r: __l.concatenate(__r), [DATASETS[__n] for __n in DATASET_KEYS])\n",
        "DATASET_DIM = DATASET_ALL.cardinality().numpy()\n",
        "\n",
        "DATASET_TEST = DATASET_ALL.take(1)\n",
        "DATASET_TRAIN = DATASET_ALL.skip(1).take(BASE_CONFIG['steps'])\n",
        "\n",
        "# RANDOM_TEST = DATASETS['random'].take(128)\n",
        "# RANDOM_TRAIN = DATASETS['random'].skip(128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ3quJQ4EUKf"
      },
      "outputs": [],
      "source": [
        "# INSPECT ######################################################################\n",
        "\n",
        "__X = next(iter(DATASET_TRAIN.take(1)))\n",
        "__V = tf.zeros(mlable.shapes.filter(__X.shape, axes=[0]), dtype=tf.float32)\n",
        "\n",
        "print(DATASET_TRAIN.element_spec)\n",
        "print(DATASET_TEST.element_spec)\n",
        "\n",
        "print('train: {:,}'.format(DATASET_TRAIN.cardinality().numpy()))\n",
        "print('test:  {:,}'.format(DATASET_TEST.cardinality().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbkaXey44V5Q"
      },
      "source": [
        "## Init The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEpY1-vFIFX7"
      },
      "outputs": [],
      "source": [
        "# COMPILE ######################################################################\n",
        "\n",
        "with DISTRIBUTION_STRATEGY.scope():\n",
        "    # metrics\n",
        "    # byte_accuracy = mlable.metrics.BinaryGroupAccuracy(group=1, name='byte_accuracy', **METRICS_CONFIG)\n",
        "    # token_accuracy = mlable.metrics.BinaryGroupAccuracy(group=BASE_CONFIG['token_dim'], name='token_accuracy', **METRICS_CONFIG)\n",
        "    # cosing LR\n",
        "    OPTIMIZER_CONFIG['learning_rate'] = tf.keras.optimizers.schedules.CosineDecay(**SCHEDULER_CONFIG)\n",
        "    # weights\n",
        "    MODEL = MODEL_FACTORY[ARCH1](**MODEL_CONFIG[ARCH1])\n",
        "    if IMPORT and os.path.isfile(IO_CONFIG['tr1cot']['path']): MODEL = tf.keras.models.load_model(IO_CONFIG['tr1cot']['path'], compile=False)\n",
        "    # vq-vae\n",
        "    TOKUN = tf.keras.models.load_model(IO_CONFIG['tokun']['path'], compile=False)\n",
        "    TOKUN.trainable = False\n",
        "    MODEL.set_vae(TOKUN)\n",
        "    # compile\n",
        "    MODEL.compile(\n",
        "        optimizer=tf.keras.optimizers.AdamW(**OPTIMIZER_CONFIG),\n",
        "        loss=tf.keras.losses.MeanAbsoluteError(reduction='sum_over_batch_size'), # tf.keras.losses.BinaryCrossentropy(**LOSS_CONFIG),\n",
        "        weighted_metrics=[]) # byte_accuracy, token_accuracy\n",
        "    # build tokun\n",
        "    TOKUN(__X, training=False)\n",
        "    # encode inputs\n",
        "    __L = TOKUN.encode(__X, training=False)\n",
        "    # build the model in the latent space\n",
        "    MODEL((__L, __V), training=False)\n",
        "    MODEL.compute_metrics((__L, __V), __L, __L)\n",
        "    MODEL.compute_loss((__L, __V), __L, __L)\n",
        "    # normalize the latent space\n",
        "    # MODEL.adapt(DATASET_TRAIN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuRwWdjpPQBM"
      },
      "outputs": [],
      "source": [
        "# INSPECT ######################################################################\n",
        "\n",
        "MODEL.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zN5DeRPbrIT-"
      },
      "outputs": [],
      "source": [
        "print(MODEL.compute_metrics((__L, __V), __L, __L))\n",
        "print(MODEL.compute_loss((__L, __V), __L, __L))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cND42HgDVlAz"
      },
      "outputs": [],
      "source": [
        "# DATAVIZ ######################################################################\n",
        "\n",
        "def unpack(data: tf.Tensor) -> list:\n",
        "    return [b'\\n'.join(__s).decode('utf-8', errors='replace') for __s in data.numpy().tolist()]\n",
        "\n",
        "def generate_samples(model: tf.keras.models.Model=MODEL, sample_num: int=1, step_num: int=8, eta_rate: float=0.1) -> str:\n",
        "    __logits = model.generate_samples(sample_num=sample_num, total_step=step_num, eta_rate=0.1)\n",
        "    __text = __postprocess_sampler(__logits)\n",
        "    # return mlable.text.unpack(__text) # 1D\n",
        "    return unpack(__text) # 2D\n",
        "\n",
        "def print_sample(epoch: int=None, logs: dict=None, step_num: int=32, model: tf.keras.models.Model=MODEL) -> None:\n",
        "    print(generate_samples(sample_num=1, step_num=step_num, model=model)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRkNkXthBwar"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beTpALmzFdu1"
      },
      "outputs": [],
      "source": [
        "# TRAIN ########################################################################\n",
        "\n",
        "if TRAINING:\n",
        "    with DISTRIBUTION_STRATEGY.scope():\n",
        "        # callbacks\n",
        "        cp_callback = tf.keras.callbacks.ModelCheckpoint(**CHECKPOINT_CONFIG)\n",
        "        tb_callback = tf.keras.callbacks.TensorBoard(**TENSORBOARD_CONFIG)\n",
        "        tn_callback = tf.keras.callbacks.TerminateOnNaN()\n",
        "        gs_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=print_sample)\n",
        "        # fit model\n",
        "        TRAINING_HISTORY = MODEL.fit(\n",
        "            x=DATASET_TRAIN.prefetch(tf.data.AUTOTUNE),\n",
        "            validation_data=DATASET_TEST.prefetch(tf.data.AUTOTUNE),\n",
        "            callbacks=[cp_callback, tb_callback, tn_callback, gs_callback],\n",
        "            **TRAINING_CONFIG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHtROW1K1R7c"
      },
      "source": [
        "## Dataviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziA7Tq8rs5gm"
      },
      "outputs": [],
      "source": [
        "# DATASET SAMPLES ##############################################################\n",
        "\n",
        "__X = next(iter(DATASET_TRAIN.take(1)))\n",
        "__Y = TOKUN(__X, logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRwOXoBg3AiB"
      },
      "outputs": [],
      "source": [
        "__O_T = unpack(__postprocess_sampler(__Y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "009P_Tm22WBI"
      },
      "outputs": [],
      "source": [
        "__i = 0\n",
        "print(__O_T[__i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKdaYXR5SyY9"
      },
      "outputs": [],
      "source": [
        "# GENERATE #####################################################################\n",
        "\n",
        "__s = generate_samples(sample_num=4, step_num=256, model=MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBmuhLy8Zlxs"
      },
      "outputs": [],
      "source": [
        "__i = 2\n",
        "print(__s[__i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfopolmD9fNx"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eJmv4xjnTH4t"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir .logs"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}